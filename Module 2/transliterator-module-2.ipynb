{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import collections\nimport helper\nimport numpy as np\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model\nfrom keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\nfrom keras.layers.embeddings import Embedding\nfrom keras.optimizers import Adam\nfrom keras.losses import sparse_categorical_crossentropy","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"## **Helper function to load dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\ndef load_data(path):\n    \"\"\"\n    Load dataset\n    \"\"\"\n    input_file = os.path.join(path)\n    with open(input_file, \"r\") as f:\n        data = f.read()\n\n    return data.split('\\n')\n\nimport numpy as np\nfrom keras.losses import sparse_categorical_crossentropy\nfrom keras.models import Sequential\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\n\n\ndef _test_model(model, input_shape, output_sequence_length, french_vocab_size):\n    if isinstance(model, Sequential):\n        model = model.model\n\n    assert model.input_shape == (None, *input_shape[1:]),'Wrong input shape. Found input shape {} using parameter input_shape={}'.format(model.input_shape, input_shape)\n\n    assert model.output_shape == (None, output_sequence_length, french_vocab_size),'Wrong output shape. Found output shape {} using parameters output_sequence_length={} and french_vocab_size={}'.format(model.output_shape, output_sequence_length, french_vocab_size)\n\n    assert len(model.loss_functions) > 0,'No loss function set.  Apply the `compile` function to the model.'\n\n    assert sparse_categorical_crossentropy in model.loss_functions,'Not using `sparse_categorical_crossentropy` function for loss.'\n\n\ndef test_tokenize(tokenize):\n    sentences = [\n        'The quick brown fox jumps over the lazy dog .',\n        'By Jove , my quick study of lexicography won a prize .',\n        'This is a short sentence .']\n    tokenized_sentences, tokenizer = tokenize(sentences)\n    assert tokenized_sentences == tokenizer.texts_to_sequences(sentences),\\\n        'Tokenizer returned and doesn\\'t generate the same sentences as the tokenized sentences returned. '\n\n\ndef test_pad(pad):\n    tokens = [\n        [i for i in range(4)],\n        [i for i in range(6)],\n        [i for i in range(3)]]\n    padded_tokens = pad(tokens)\n    padding_id = padded_tokens[0][-1]\n    true_padded_tokens = np.array([\n        [i for i in range(4)] + [padding_id]*2,\n        [i for i in range(6)],\n        [i for i in range(3)] + [padding_id]*3])\n    assert isinstance(padded_tokens, np.ndarray),\\\n        'Pad returned the wrong type.  Found {} type, expected numpy array type.'\n    assert np.all(padded_tokens == true_padded_tokens), 'Pad returned the wrong results.'\n\n    padded_tokens_using_length = pad(tokens, 9)\n    assert np.all(padded_tokens_using_length == np.concatenate((true_padded_tokens, np.full((3, 3), padding_id)), axis=1)),\\\n        'Using length argument return incorrect results'\n\n\ndef test_simple_model(simple_model):\n    input_shape = (137861, 21, 1)\n    output_sequence_length = 21\n    english_vocab_size = 199\n    french_vocab_size = 344\n\n    model = simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n\n\ndef test_embed_model(embed_model):\n    input_shape = (137861, 21)\n    output_sequence_length = 21\n    english_vocab_size = 199\n    french_vocab_size = 344\n\n    model = embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n\n\ndef test_encdec_model(encdec_model):\n    input_shape = (137861, 15, 1)\n    output_sequence_length = 21\n    english_vocab_size = 199\n    french_vocab_size = 344\n\n    model = encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n\n\ndef test_bd_model(bd_model):\n    input_shape = (137861, 21, 1)\n    output_sequence_length = 21\n    english_vocab_size = 199\n    french_vocab_size = 344\n\n    model = bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n\n\ndef test_model_final(model_final):\n    input_shape = (137861, 15)\n    output_sequence_length = 21\n    english_vocab_size = 199\n    french_vocab_size = 344\n\n    model = model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n    _test_model(model, input_shape, output_sequence_length, french_vocab_size)","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Loading the Dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"english_sentences = load_data('../input/ai-project/english_sentences.txt')\nfrench_sentences = load_data('../input/ai-project/french_sentences.txt')\nprint('Dataset Loaded')","execution_count":3,"outputs":[{"output_type":"stream","text":"Dataset Loaded\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## **Analysing and Preprocessing the Vocab of the dataset**. "},{"metadata":{"trusted":true},"cell_type":"code","source":"english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\nfrench_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\nprint('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\nprint('{} unique English words.'.format(len(english_words_counter)))\nprint('10 Most common words in the English dataset:')\nprint('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\nprint()\nprint('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\nprint('{} unique French words.'.format(len(french_words_counter)))\nprint('10 Most common words in the French dataset:')\nprint('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')","execution_count":4,"outputs":[{"output_type":"stream","text":"2679573 English words.\n12808 unique English words.\n10 Most common words in the English dataset:\n\"is\" \",\" \".\" \"the\" \"it\" \"in\" \"during\" \"but\" \"and\" \"i\"\n\n2865380 French words.\n25481 unique French words.\n10 Most common words in the French dataset:\n\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"la\" \"mais\" \"et\" \"le\"\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### **Tokenizing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(x):\n    x_tk = Tokenizer(char_level = False)\n    x_tk.fit_on_texts(x)\n    return x_tk.texts_to_sequences(x), x_tk\ntext_sentences = [\n    'The quick brown fox jumps over the lazy dog .',\n    'By Jove , my quick study of lexicography won a prize .',\n    'This is a short sentence .']\ntext_tokenized, text_tokenizer = tokenize(text_sentences)\nprint(text_tokenizer.word_index)\nprint()\nfor sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n    print('Sequence {} in x'.format(sample_i + 1))\n    print('  Input:  {}'.format(sent))\n    print('  Output: {}'.format(token_sent))","execution_count":5,"outputs":[{"output_type":"stream","text":"{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n\nSequence 1 in x\n  Input:  The quick brown fox jumps over the lazy dog .\n  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\nSequence 2 in x\n  Input:  By Jove , my quick study of lexicography won a prize .\n  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\nSequence 3 in x\n  Input:  This is a short sentence .\n  Output: [18, 19, 3, 20, 21]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### **Padding**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import project_tests as tests\ndef pad(x, length=None):\n    if length is None:\n        length = max([len(sentence) for sentence in x])\n    return pad_sequences(x, maxlen = length, padding = 'post')\ntest_pad(pad)\n# Pad Tokenized output\ntest_pad = pad(text_tokenized)\nfor sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n    print('Sequence {} in x'.format(sample_i + 1))\n    print('  Input:  {}'.format(np.array(token_sent)))\n    print('  Output: {}'.format(pad_sent))","execution_count":6,"outputs":[{"output_type":"stream","text":"Sequence 1 in x\n  Input:  [1 2 4 5 6 7 1 8 9]\n  Output: [1 2 4 5 6 7 1 8 9 0]\nSequence 2 in x\n  Input:  [10 11 12  2 13 14 15 16  3 17]\n  Output: [10 11 12  2 13 14 15 16  3 17]\nSequence 3 in x\n  Input:  [18 19  3 20 21]\n  Output: [18 19  3 20 21  0  0  0  0  0]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### **Preprocess Pipe-line**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(x, y):\n    preprocess_x, x_tk = tokenize(x)\n    preprocess_y, y_tk = tokenize(y)\n    preprocess_x = pad(preprocess_x)\n    preprocess_y = pad(preprocess_y)\n    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n    return preprocess_x, preprocess_y, x_tk, y_tk\n\npreproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer = preprocess(english_sentences, french_sentences)    \nmax_english_sequence_length = preproc_english_sentences.shape[1]\nmax_french_sequence_length = preproc_french_sentences.shape[1]\nenglish_vocab_size = len(english_tokenizer.word_index)\nfrench_vocab_size = len(french_tokenizer.word_index)\nprint('Data Preprocessed')\nprint(\"Max English sentence length:\", max_english_sequence_length)\nprint(\"Max French sentence length:\", max_french_sequence_length)\nprint(\"English vocabulary size:\", english_vocab_size)\nprint(\"French vocabulary size:\", french_vocab_size)","execution_count":7,"outputs":[{"output_type":"stream","text":"Data Preprocessed\nMax English sentence length: 15\nMax French sentence length: 21\nEnglish vocabulary size: 12780\nFrench vocabulary size: 25465\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### **Converting the Ids back to text**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def logits_to_text(logits, tokenizer):\n    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n    index_to_words[0] = '<PAD>'\n    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\nprint('`logits_to_text` function loaded.')","execution_count":8,"outputs":[{"output_type":"stream","text":"`logits_to_text` function loaded.\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## **Training Models** "},{"metadata":{},"cell_type":"markdown","source":"#### **Simple Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n    learning_rate = 1e-3\n    input_seq = Input(input_shape[1:])\n    rnn = GRU(64, return_sequences = True)(input_seq)\n    logits = TimeDistributed(Dense(french_vocab_size))(rnn)\n    model = Model(input_seq, Activation('softmax')(logits))\n    model.compile(loss = sparse_categorical_crossentropy, \n                 optimizer = Adam(learning_rate), \n                 metrics = ['accuracy'])\n    \n    return model\n# test_simple_model(simple_model)\ntmp_x = pad(preproc_english_sentences, max_french_sequence_length)\ntmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n# Train the neural network\nsimple_rnn_model = simple_model(\n    tmp_x.shape,\n    max_french_sequence_length,\n    english_vocab_size+1,\n    french_vocab_size+1)\nsimple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n# Print prediction(s)\nprint(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))","execution_count":9,"outputs":[{"output_type":"stream","text":"Train on 226289 samples, validate on 56573 samples\nEpoch 1/10\n226289/226289 [==============================] - 116s 511us/step - loss: 4.2116 - accuracy: 0.5889 - val_loss: 3.3431 - val_accuracy: 0.4087\nEpoch 2/10\n226289/226289 [==============================] - 114s 503us/step - loss: 2.6399 - accuracy: 0.5978 - val_loss: 2.9900 - val_accuracy: 0.4093\nEpoch 3/10\n226289/226289 [==============================] - 114s 504us/step - loss: 2.4394 - accuracy: 0.6023 - val_loss: 2.6809 - val_accuracy: 0.4145\nEpoch 4/10\n226289/226289 [==============================] - 114s 504us/step - loss: 2.2953 - accuracy: 0.6200 - val_loss: 2.4438 - val_accuracy: 0.4807\nEpoch 5/10\n226289/226289 [==============================] - 114s 504us/step - loss: 2.1804 - accuracy: 0.6353 - val_loss: 2.2223 - val_accuracy: 0.5083\nEpoch 6/10\n226289/226289 [==============================] - 114s 503us/step - loss: 2.0915 - accuracy: 0.6387 - val_loss: 2.0670 - val_accuracy: 0.5096\nEpoch 7/10\n226289/226289 [==============================] - 114s 504us/step - loss: 2.0303 - accuracy: 0.6429 - val_loss: 1.9541 - val_accuracy: 0.5241\nEpoch 8/10\n226289/226289 [==============================] - 114s 504us/step - loss: 1.9746 - accuracy: 0.6540 - val_loss: 1.8247 - val_accuracy: 0.5574\nEpoch 9/10\n226289/226289 [==============================] - 114s 504us/step - loss: 1.9263 - accuracy: 0.6626 - val_loss: 1.7447 - val_accuracy: 0.5709\nEpoch 10/10\n226289/226289 [==============================] - 114s 503us/step - loss: 1.8919 - accuracy: 0.6676 - val_loss: 1.6793 - val_accuracy: 0.5805\nnous est <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### RNN with Embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\ndef embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n    learning_rate = 1e-3\n    rnn = GRU(64, return_sequences=True, activation=\"tanh\")\n    \n    embedding = Embedding(french_vocab_size, 64, input_length=input_shape[1]) \n    logits = TimeDistributed(Dense(french_vocab_size, activation=\"softmax\"))\n    \n    model = Sequential()\n    #em can only be used in first layer --> Keras Documentation\n    model.add(embedding)\n    model.add(rnn)\n    model.add(logits)\n    model.compile(loss=sparse_categorical_crossentropy,\n                  optimizer=Adam(learning_rate),\n                  metrics=['accuracy'])\n    \n    return model\n# tests.test_embed_model(embed_model)\ntmp_x = pad(preproc_english_sentences, max_french_sequence_length)\ntmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\nembeded_model = embed_model(\n    tmp_x.shape,\n    max_french_sequence_length,\n    english_vocab_size+1,\n    french_vocab_size+1)\nembeded_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\nprint(logits_to_text(embeded_model.predict(tmp_x[:1])[0], french_tokenizer))","execution_count":10,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","name":"stderr"},{"output_type":"stream","text":"Train on 226289 samples, validate on 56573 samples\nEpoch 1/10\n226289/226289 [==============================] - 86s 379us/step - loss: 4.4887 - accuracy: 0.5912 - val_loss: 3.4840 - val_accuracy: 0.4087\nEpoch 2/10\n226289/226289 [==============================] - 85s 375us/step - loss: 2.6673 - accuracy: 0.6047 - val_loss: 2.7944 - val_accuracy: 0.4600\nEpoch 3/10\n226289/226289 [==============================] - 85s 376us/step - loss: 2.2934 - accuracy: 0.6321 - val_loss: 2.1872 - val_accuracy: 0.5065\nEpoch 4/10\n226289/226289 [==============================] - 85s 375us/step - loss: 1.9704 - accuracy: 0.6613 - val_loss: 1.7187 - val_accuracy: 0.5817\nEpoch 5/10\n226289/226289 [==============================] - 85s 375us/step - loss: 1.7781 - accuracy: 0.6929 - val_loss: 1.4416 - val_accuracy: 0.6422\nEpoch 6/10\n226289/226289 [==============================] - 85s 376us/step - loss: 1.6258 - accuracy: 0.7182 - val_loss: 1.1939 - val_accuracy: 0.7015\nEpoch 7/10\n226289/226289 [==============================] - 85s 375us/step - loss: 1.5016 - accuracy: 0.7417 - val_loss: 1.0212 - val_accuracy: 0.7408\nEpoch 8/10\n226289/226289 [==============================] - 85s 376us/step - loss: 1.4098 - accuracy: 0.7560 - val_loss: 0.8967 - val_accuracy: 0.7700\nEpoch 9/10\n226289/226289 [==============================] - 85s 375us/step - loss: 1.3354 - accuracy: 0.7682 - val_loss: 0.8011 - val_accuracy: 0.7940\nEpoch 10/10\n226289/226289 [==============================] - 85s 376us/step - loss: 1.2731 - accuracy: 0.7773 - val_loss: 0.7234 - val_accuracy: 0.8093\nla <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### **Bidirectional RNN**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n   \n    learning_rate = 1e-3\n    model = Sequential()\n    model.add(Bidirectional(GRU(128, return_sequences = True, dropout = 0.1), \n                           input_shape = input_shape[1:]))\n    model.add(TimeDistributed(Dense(french_vocab_size, activation = 'softmax')))\n    model.compile(loss = sparse_categorical_crossentropy, \n                 optimizer = Adam(learning_rate), \n                 metrics = ['accuracy'])\n    return model\n# tests.test_bd_model(bd_model)\ntmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\ntmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\nbidi_model = bd_model(\n    tmp_x.shape,\n    preproc_french_sentences.shape[1],\n    len(english_tokenizer.word_index)+1,\n    len(french_tokenizer.word_index)+1)\nbidi_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=20, validation_split=0.2)\n# Print prediction(s)\nprint(logits_to_text(bidi_model.predict(tmp_x[:1])[0], french_tokenizer))","execution_count":11,"outputs":[{"output_type":"stream","text":"Train on 226289 samples, validate on 56573 samples\nEpoch 1/20\n226289/226289 [==============================] - 109s 482us/step - loss: 3.2567 - accuracy: 0.5996 - val_loss: 2.4549 - val_accuracy: 0.4651\nEpoch 2/20\n226289/226289 [==============================] - 108s 476us/step - loss: 2.1960 - accuracy: 0.6343 - val_loss: 1.9765 - val_accuracy: 0.5348\nEpoch 3/20\n226289/226289 [==============================] - 108s 476us/step - loss: 1.9890 - accuracy: 0.6569 - val_loss: 1.6207 - val_accuracy: 0.5891\nEpoch 4/20\n226289/226289 [==============================] - 108s 477us/step - loss: 1.8656 - accuracy: 0.6745 - val_loss: 1.4722 - val_accuracy: 0.6104\nEpoch 5/20\n226289/226289 [==============================] - 108s 478us/step - loss: 1.7858 - accuracy: 0.6848 - val_loss: 1.3886 - val_accuracy: 0.6239\nEpoch 6/20\n226289/226289 [==============================] - 108s 476us/step - loss: 1.7272 - accuracy: 0.6936 - val_loss: 1.3427 - val_accuracy: 0.6289\nEpoch 7/20\n226289/226289 [==============================] - 108s 477us/step - loss: 1.6815 - accuracy: 0.7005 - val_loss: 1.3055 - val_accuracy: 0.6386\nEpoch 8/20\n226289/226289 [==============================] - 108s 477us/step - loss: 1.6424 - accuracy: 0.7056 - val_loss: 1.2885 - val_accuracy: 0.6390\nEpoch 9/20\n226289/226289 [==============================] - 108s 478us/step - loss: 1.6100 - accuracy: 0.7088 - val_loss: 1.3024 - val_accuracy: 0.6358\nEpoch 10/20\n226289/226289 [==============================] - 108s 477us/step - loss: 1.5822 - accuracy: 0.7113 - val_loss: 1.3071 - val_accuracy: 0.6370\nEpoch 11/20\n226289/226289 [==============================] - 108s 477us/step - loss: 1.5587 - accuracy: 0.7137 - val_loss: 1.3432 - val_accuracy: 0.6309\nEpoch 12/20\n226289/226289 [==============================] - 108s 477us/step - loss: 1.5375 - accuracy: 0.7158 - val_loss: 1.3558 - val_accuracy: 0.6322\nEpoch 13/20\n226289/226289 [==============================] - 108s 478us/step - loss: 1.5203 - accuracy: 0.7175 - val_loss: 1.3902 - val_accuracy: 0.6271\nEpoch 14/20\n226289/226289 [==============================] - 108s 478us/step - loss: 1.5027 - accuracy: 0.7195 - val_loss: 1.4251 - val_accuracy: 0.6291\nEpoch 15/20\n226289/226289 [==============================] - 108s 479us/step - loss: 1.4891 - accuracy: 0.7207 - val_loss: 1.4403 - val_accuracy: 0.6254\nEpoch 16/20\n226289/226289 [==============================] - 108s 477us/step - loss: 1.4748 - accuracy: 0.7223 - val_loss: 1.4597 - val_accuracy: 0.6304\nEpoch 17/20\n226289/226289 [==============================] - 108s 477us/step - loss: 1.4626 - accuracy: 0.7236 - val_loss: 1.4800 - val_accuracy: 0.6301\nEpoch 18/20\n226289/226289 [==============================] - 108s 478us/step - loss: 1.4510 - accuracy: 0.7247 - val_loss: 1.4907 - val_accuracy: 0.6298\nEpoch 19/20\n226289/226289 [==============================] - 108s 478us/step - loss: 1.4408 - accuracy: 0.7258 - val_loss: 1.5088 - val_accuracy: 0.6267\nEpoch 20/20\n226289/226289 [==============================] - 108s 478us/step - loss: 1.4293 - accuracy: 0.7269 - val_loss: 1.5639 - val_accuracy: 0.6237\nils est <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### Encoder-Decoder RNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n  \n    learning_rate = 1e-3\n    model = Sequential()\n    model.add(GRU(128, input_shape = input_shape[1:], return_sequences = False))\n    model.add(RepeatVector(output_sequence_length))\n    model.add(GRU(128, return_sequences = True))\n    model.add(TimeDistributed(Dense(french_vocab_size, activation = 'softmax')))\n    \n    model.compile(loss = sparse_categorical_crossentropy, \n                 optimizer = Adam(learning_rate), \n                 metrics = ['accuracy'])\n    return model\n# tests.test_encdec_model(encdec_model)\ntmp_x = pad(preproc_english_sentences)\ntmp_x = tmp_x.reshape((-1, preproc_english_sentences.shape[1], 1))\nencodeco_model = encdec_model(\n    tmp_x.shape,\n    preproc_french_sentences.shape[1],\n    len(english_tokenizer.word_index)+1,\n    len(french_tokenizer.word_index)+1)\nencodeco_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=20, validation_split=0.2)\nprint(logits_to_text(encodeco_model.predict(tmp_x[:1])[0], french_tokenizer))\n","execution_count":12,"outputs":[{"output_type":"stream","text":"Train on 226289 samples, validate on 56573 samples\nEpoch 1/20\n226289/226289 [==============================] - 98s 433us/step - loss: 3.5433 - accuracy: 0.5944 - val_loss: 2.9994 - val_accuracy: 0.4087\nEpoch 2/20\n226289/226289 [==============================] - 97s 428us/step - loss: 2.4092 - accuracy: 0.6144 - val_loss: 2.5597 - val_accuracy: 0.4693\nEpoch 3/20\n226289/226289 [==============================] - 97s 428us/step - loss: 2.2389 - accuracy: 0.6298 - val_loss: 2.3773 - val_accuracy: 0.4935\nEpoch 4/20\n226289/226289 [==============================] - 97s 428us/step - loss: 2.1531 - accuracy: 0.6379 - val_loss: 2.2556 - val_accuracy: 0.5041\nEpoch 5/20\n226289/226289 [==============================] - 97s 428us/step - loss: 2.0977 - accuracy: 0.6423 - val_loss: 2.1329 - val_accuracy: 0.5140\nEpoch 6/20\n226289/226289 [==============================] - 97s 429us/step - loss: 2.0224 - accuracy: 0.6466 - val_loss: 1.9626 - val_accuracy: 0.5223\nEpoch 7/20\n226289/226289 [==============================] - 97s 430us/step - loss: 1.9634 - accuracy: 0.6515 - val_loss: 1.8462 - val_accuracy: 0.5502\nEpoch 8/20\n226289/226289 [==============================] - 97s 429us/step - loss: 1.9204 - accuracy: 0.6613 - val_loss: 1.7679 - val_accuracy: 0.5676\nEpoch 9/20\n226289/226289 [==============================] - 97s 428us/step - loss: 1.8861 - accuracy: 0.6645 - val_loss: 1.7114 - val_accuracy: 0.5688\nEpoch 10/20\n226289/226289 [==============================] - 97s 428us/step - loss: 1.8557 - accuracy: 0.6678 - val_loss: 1.6444 - val_accuracy: 0.5821\nEpoch 11/20\n226289/226289 [==============================] - 97s 428us/step - loss: 1.8222 - accuracy: 0.6717 - val_loss: 1.5831 - val_accuracy: 0.5868\nEpoch 12/20\n226289/226289 [==============================] - 97s 428us/step - loss: 1.7887 - accuracy: 0.6750 - val_loss: 1.5140 - val_accuracy: 0.5987\nEpoch 13/20\n226289/226289 [==============================] - 97s 429us/step - loss: 1.7502 - accuracy: 0.6807 - val_loss: 1.4442 - val_accuracy: 0.6127\nEpoch 14/20\n226289/226289 [==============================] - 97s 427us/step - loss: 1.7153 - accuracy: 0.6862 - val_loss: 1.3963 - val_accuracy: 0.6232\nEpoch 15/20\n226289/226289 [==============================] - 97s 428us/step - loss: 1.6877 - accuracy: 0.6907 - val_loss: 1.3559 - val_accuracy: 0.6326\nEpoch 16/20\n226289/226289 [==============================] - 97s 428us/step - loss: 1.6637 - accuracy: 0.6944 - val_loss: 1.3330 - val_accuracy: 0.6354\nEpoch 17/20\n226289/226289 [==============================] - 97s 429us/step - loss: 1.6424 - accuracy: 0.6976 - val_loss: 1.3000 - val_accuracy: 0.6440\nEpoch 18/20\n226289/226289 [==============================] - 97s 428us/step - loss: 1.6205 - accuracy: 0.7007 - val_loss: 1.2745 - val_accuracy: 0.6484\nEpoch 19/20\n226289/226289 [==============================] - 97s 427us/step - loss: 1.6012 - accuracy: 0.7036 - val_loss: 1.2297 - val_accuracy: 0.6598\nEpoch 20/20\n226289/226289 [==============================] - 97s 429us/step - loss: 1.5808 - accuracy: 0.7071 - val_loss: 1.1940 - val_accuracy: 0.6686\ncest <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### Custom Model (Embedded + Bidirectional)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n  \n    model = Sequential()\n    model.add(Embedding(input_dim=english_vocab_size,output_dim=128,input_length=input_shape[1]))\n    model.add(Bidirectional(GRU(256,return_sequences=False)))\n    model.add(RepeatVector(output_sequence_length))\n    model.add(Bidirectional(GRU(256,return_sequences=True)))\n    model.add(TimeDistributed(Dense(french_vocab_size,activation='softmax')))\n    learning_rate = 0.005\n    \n    model.compile(loss = sparse_categorical_crossentropy, \n                 optimizer = Adam(learning_rate), \n                 metrics = ['accuracy'])\n    \n    return model\n# tests.test_model_final(model_final)\nprint('Final Model Loaded')","execution_count":13,"outputs":[{"output_type":"stream","text":"Final Model Loaded\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp_X = pad(preproc_english_sentences)\nmodel = model_final(tmp_X.shape,preproc_french_sentences.shape[1],len(english_tokenizer.word_index)+1,len(french_tokenizer.word_index)+1)\nmodel.fit(tmp_X, preproc_french_sentences, batch_size = 1024, epochs = 20, validation_split = 0.2)\nmodel.save('Finalmodel.model')","execution_count":14,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","name":"stderr"},{"output_type":"stream","text":"Train on 226289 samples, validate on 56573 samples\nEpoch 1/20\n226289/226289 [==============================] - 152s 672us/step - loss: 2.6912 - accuracy: 0.6169 - val_loss: 2.2275 - val_accuracy: 0.5145\nEpoch 2/20\n226289/226289 [==============================] - 149s 658us/step - loss: 2.1393 - accuracy: 0.6551 - val_loss: 1.7768 - val_accuracy: 0.5586\nEpoch 3/20\n226289/226289 [==============================] - 149s 660us/step - loss: 1.9695 - accuracy: 0.6742 - val_loss: 1.5155 - val_accuracy: 0.6022\nEpoch 4/20\n226289/226289 [==============================] - 149s 660us/step - loss: 1.8686 - accuracy: 0.6907 - val_loss: 1.3702 - val_accuracy: 0.6275\nEpoch 5/20\n226289/226289 [==============================] - 149s 659us/step - loss: 1.7771 - accuracy: 0.7064 - val_loss: 1.2112 - val_accuracy: 0.6639\nEpoch 6/20\n226289/226289 [==============================] - 149s 657us/step - loss: 1.6965 - accuracy: 0.7213 - val_loss: 1.0611 - val_accuracy: 0.7084\nEpoch 7/20\n226289/226289 [==============================] - 149s 659us/step - loss: 1.6441 - accuracy: 0.7318 - val_loss: 0.9871 - val_accuracy: 0.7229\nEpoch 8/20\n226289/226289 [==============================] - 149s 659us/step - loss: 1.5745 - accuracy: 0.7451 - val_loss: 0.9212 - val_accuracy: 0.7435\nEpoch 9/20\n226289/226289 [==============================] - 149s 659us/step - loss: 1.5643 - accuracy: 0.7477 - val_loss: 0.8794 - val_accuracy: 0.7599\nEpoch 10/20\n226289/226289 [==============================] - 149s 659us/step - loss: 1.4795 - accuracy: 0.7634 - val_loss: 0.8120 - val_accuracy: 0.7725\nEpoch 11/20\n226289/226289 [==============================] - 149s 658us/step - loss: 1.4443 - accuracy: 0.7685 - val_loss: 0.7645 - val_accuracy: 0.7854\nEpoch 12/20\n226289/226289 [==============================] - 149s 659us/step - loss: 1.4018 - accuracy: 0.7762 - val_loss: 0.7159 - val_accuracy: 0.7988\nEpoch 13/20\n226289/226289 [==============================] - 149s 659us/step - loss: 1.3637 - accuracy: 0.7835 - val_loss: 0.6856 - val_accuracy: 0.8075\nEpoch 14/20\n226289/226289 [==============================] - 149s 659us/step - loss: 1.3449 - accuracy: 0.7864 - val_loss: 0.6780 - val_accuracy: 0.8107\nEpoch 15/20\n226289/226289 [==============================] - 149s 659us/step - loss: 1.3029 - accuracy: 0.7949 - val_loss: 0.6354 - val_accuracy: 0.8250\nEpoch 16/20\n226289/226289 [==============================] - 149s 660us/step - loss: 1.2756 - accuracy: 0.7997 - val_loss: 0.5667 - val_accuracy: 0.8437\nEpoch 17/20\n226289/226289 [==============================] - 149s 659us/step - loss: 1.2890 - accuracy: 0.7969 - val_loss: 0.5562 - val_accuracy: 0.8472\nEpoch 18/20\n226289/226289 [==============================] - 149s 659us/step - loss: 1.2192 - accuracy: 0.8106 - val_loss: 0.5039 - val_accuracy: 0.8617\nEpoch 19/20\n226289/226289 [==============================] - 149s 658us/step - loss: 1.2004 - accuracy: 0.8141 - val_loss: 0.4732 - val_accuracy: 0.8721\nEpoch 20/20\n226289/226289 [==============================] - 149s 658us/step - loss: 1.1654 - accuracy: 0.8210 - val_loss: 0.4380 - val_accuracy: 0.8812\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def final_predictions(x, y, x_tk, y_tk):\n    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n    y_id_to_word[0] = '<PAD>'\n    sentence = 'he saw a old yellow truck'\n    sentence = [x_tk.word_index[word] for word in sentence.split()]\n    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n    sentences = np.array([sentence[0], x[0]])\n    predictions = model.predict(sentences, len(sentences))\n    print('Sample 1:')\n    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n    print('Il a vu un vieux camion jaune')\n    print('Sample 2:')\n    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n    print(' '.join([y_id_to_word[np.max(x)] for x in y[0]]))\nfinal_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)","execution_count":15,"outputs":[{"output_type":"stream","text":"Sample 1:\nil a vu un vieux camion jaune <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\nIl a vu un vieux camion jaune\nSample 2:\nva <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\nva <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","name":"stdout"}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}